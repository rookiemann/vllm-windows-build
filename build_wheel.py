"""
Package a compiled vLLM installation into a distributable .whl file.

Walks the vllm/ package directory from an existing build, collects all
Python sources, compiled extensions (.pyd), and data files, then
produces a PEP 427 wheel with correct METADATA, WHEEL tag, and RECORD.

Usage:
    python build_wheel.py --source-dir E:\\AgentNate\\vllm-source --output-dir dist
"""

import argparse
import csv
import hashlib
import io
import os
import re
import sys
import zipfile
from base64 import urlsafe_b64encode
from pathlib import Path

VERSION = "0.14.2+win.cu126"
PYTHON_TAG = "cp310"
ABI_TAG = "cp310"
PLATFORM_TAG = "win_amd64"

DIST_NAME = "vllm"
WHEEL_FILENAME = f"{DIST_NAME}-{VERSION}-{PYTHON_TAG}-{ABI_TAG}-{PLATFORM_TAG}.whl"
DIST_INFO = f"{DIST_NAME}-{VERSION}.dist-info"

# File extensions to include in the wheel
INCLUDE_EXTENSIONS = {
    ".py", ".pyd", ".pyi", ".json", ".js", ".css", ".proto",
    ".typed", ".txt", ".yaml", ".yml", ".html", ".jinja",
    ".jinja2", ".j2", ".cfg", ".ini", ".toml",
}

# Names/patterns to always include regardless of extension
INCLUDE_NAMES = {"py.typed"}

# Directories to skip entirely
EXCLUDE_DIRS = {"__pycache__", "benchmarks", ".git", ".github"}

# File patterns to skip
EXCLUDE_PATTERNS = {".pyc", ".pyo"}

# Minimal Windows-compatible dependencies.
# Excludes: flashinfer (not available), ray (multiprocess), triton (linux),
# numba (optional), and other linux-only packages.
REQUIRES_DIST = [
    "torch == 2.9.1",
    "numpy",
    "transformers >= 4.56.0, < 5",
    "tokenizers >= 0.21.1",
    "fastapi[standard] >= 0.115.0",
    "uvicorn",
    "pydantic >= 2.12.0",
    "requests >= 2.26.0",
    "aiohttp",
    "tqdm",
    "sentencepiece",
    "protobuf >= 6.30.0",
    "regex",
    "pillow",
    "filelock >= 3.16.1",
    "psutil",
    "py-cpuinfo",
    "openai >= 1.99.1",
    "tiktoken >= 0.6.0",
    "prometheus-client >= 0.18.0",
    "prometheus-fastapi-instrumentator >= 7.0.0",
    "typing_extensions >= 4.10",
    "pyzmq >= 25.0.0",
    "msgspec",
    "gguf >= 0.17.0",
    "pyyaml",
    "cachetools",
    "blake3",
    "compressed-tensors == 0.13.0",
    "cloudpickle",
    "einops",
    "partial-json-parser",
    "lm-format-enforcer == 0.11.3",
    "outlines_core == 0.2.11",
    "diskcache == 5.6.3",
    "lark == 1.2.2",
    "mistral_common[image] >= 1.8.8",
    "opencv-python-headless >= 4.13.0",
    "depyf == 0.20.0",
    "setuptools >= 77.0.3, < 81.0.0",
    "pybase64",
]


def sha256_digest(data: bytes) -> str:
    """Return urlsafe base64-encoded SHA256 digest (no padding) per PEP 376."""
    h = hashlib.sha256(data)
    return urlsafe_b64encode(h.digest()).rstrip(b"=").decode("ascii")


def make_version_py(version: str) -> str:
    """Generate _version.py content with the wheel version."""
    parts = version.split("+")[0].split(".")
    local = version.split("+")[1] if "+" in version else ""
    tuple_parts = ", ".join(parts)
    if local:
        tuple_parts += f", '{local}'"
    return f'''# Generated by build_wheel.py for wheel distribution

__all__ = [
    "__version__",
    "__version_tuple__",
    "version",
    "version_tuple",
    "__commit_id__",
    "commit_id",
]

TYPE_CHECKING = False
if TYPE_CHECKING:
    from typing import Tuple, Union
    VERSION_TUPLE = Tuple[Union[int, str], ...]
    COMMIT_ID = Union[str, None]
else:
    VERSION_TUPLE = object
    COMMIT_ID = object

version: str
__version__: str
__version_tuple__: VERSION_TUPLE
version_tuple: VERSION_TUPLE
commit_id: COMMIT_ID
__commit_id__: COMMIT_ID

__version__ = version = {version!r}
__version_tuple__ = version_tuple = ({tuple_parts})

__commit_id__ = commit_id = None
'''


def generate_metadata() -> str:
    """Generate METADATA file content per PEP 566."""
    lines = [
        "Metadata-Version: 2.1",
        f"Name: {DIST_NAME}",
        f"Version: {VERSION}",
        "Summary: A high-throughput and memory-efficient inference and serving engine for LLMs (Windows build)",
        "Author: vLLM Team",
        "License: Apache-2.0",
        "Requires-Python: >=3.10,<3.11",
        f"Platform: {PLATFORM_TAG}",
    ]
    for dep in REQUIRES_DIST:
        lines.append(f"Requires-Dist: {dep}")
    return "\n".join(lines) + "\n"


def generate_wheel_tag() -> str:
    """Generate WHEEL file content per PEP 427."""
    return (
        "Wheel-Version: 1.0\n"
        "Generator: build_wheel.py\n"
        "Root-Is-Purelib: false\n"
        f"Tag: {PYTHON_TAG}-{ABI_TAG}-{PLATFORM_TAG}\n"
    )


def generate_toplevel() -> str:
    """Generate top_level.txt."""
    return "vllm\n"


def generate_entry_points() -> str:
    """Generate entry_points.txt for the vllm CLI."""
    return "[console_scripts]\nvllm = vllm.entrypoints.cli.main:main\n"


def should_include_file(rel_path: str, name: str) -> bool:
    """Check if a file should be included in the wheel."""
    # Check excluded patterns
    for pattern in EXCLUDE_PATTERNS:
        if name.endswith(pattern):
            return False

    # Always include certain names
    if name in INCLUDE_NAMES:
        return True

    # Check extension
    _, ext = os.path.splitext(name)
    return ext.lower() in INCLUDE_EXTENSIONS


def collect_package_files(vllm_dir: Path) -> list[tuple[str, Path]]:
    """
    Walk the vllm package directory and collect files for the wheel.
    Returns list of (archive_path, filesystem_path) tuples.
    """
    files = []
    vllm_dir = vllm_dir.resolve()

    for root, dirs, filenames in os.walk(vllm_dir):
        # Filter out excluded directories in-place
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for name in filenames:
            if not should_include_file(os.path.join(root, name), name):
                continue

            full_path = Path(root) / name
            # Archive path is relative to parent of vllm/
            rel = full_path.relative_to(vllm_dir.parent)
            archive_path = str(rel).replace("\\", "/")
            files.append((archive_path, full_path))

    return files


def build_wheel(source_dir: Path, output_dir: Path) -> Path:
    """Build the wheel file and return its path."""
    vllm_dir = source_dir / "vllm"
    if not vllm_dir.is_dir():
        print(f"ERROR: {vllm_dir} is not a directory")
        sys.exit(1)

    # Check for key compiled files
    key_files = ["_C.pyd", "_moe_C.pyd", "cumem_allocator.pyd"]
    for kf in key_files:
        if not (vllm_dir / kf).exists():
            print(f"WARNING: Expected compiled artifact {kf} not found in {vllm_dir}")

    print(f"Collecting files from {vllm_dir}...")
    package_files = collect_package_files(vllm_dir)
    print(f"  Found {len(package_files)} files")

    # Count by type
    ext_counts: dict[str, int] = {}
    total_size = 0
    for archive_path, fs_path in package_files:
        ext = os.path.splitext(archive_path)[1] or "(no ext)"
        ext_counts[ext] = ext_counts.get(ext, 0) + 1
        total_size += fs_path.stat().st_size
    for ext, count in sorted(ext_counts.items()):
        print(f"    {ext}: {count}")
    print(f"  Total uncompressed: {total_size / (1024**3):.2f} GB")

    # Prepare output
    output_dir.mkdir(parents=True, exist_ok=True)
    wheel_path = output_dir / WHEEL_FILENAME
    print(f"\nBuilding wheel: {wheel_path}")

    # RECORD entries: list of (path, hash, size)
    record_entries: list[tuple[str, str, int]] = []

    with zipfile.ZipFile(wheel_path, "w", zipfile.ZIP_DEFLATED) as zf:
        # Write package files
        for i, (archive_path, fs_path) in enumerate(package_files):
            data = fs_path.read_bytes()

            # Patch _version.py in-memory
            if archive_path == "vllm/_version.py":
                data = make_version_py(VERSION).encode("utf-8")
                print(f"  Patched _version.py -> version {VERSION}")

            zf.writestr(archive_path, data)
            record_entries.append((archive_path, sha256_digest(data), len(data)))

            if (i + 1) % 500 == 0:
                print(f"  ... {i + 1}/{len(package_files)} files written")

        # Write dist-info files
        metadata_content = generate_metadata().encode("utf-8")
        wheel_content = generate_wheel_tag().encode("utf-8")
        toplevel_content = generate_toplevel().encode("utf-8")
        entrypoints_content = generate_entry_points().encode("utf-8")

        dist_info_files = [
            (f"{DIST_INFO}/METADATA", metadata_content),
            (f"{DIST_INFO}/WHEEL", wheel_content),
            (f"{DIST_INFO}/top_level.txt", toplevel_content),
            (f"{DIST_INFO}/entry_points.txt", entrypoints_content),
        ]

        for path, content in dist_info_files:
            zf.writestr(path, content)
            record_entries.append((path, sha256_digest(content), len(content)))

        # Write RECORD (itself has no hash per PEP 376)
        record_path = f"{DIST_INFO}/RECORD"
        record_buf = io.StringIO()
        writer = csv.writer(record_buf, lineterminator="\n")
        for path, digest, size in record_entries:
            writer.writerow([path, f"sha256={digest}", str(size)])
        # RECORD's own entry has empty hash and size
        writer.writerow([record_path, "", ""])
        record_data = record_buf.getvalue().encode("utf-8")
        zf.writestr(record_path, record_data)

    wheel_size = wheel_path.stat().st_size
    print(f"\nWheel built successfully!")
    print(f"  {wheel_path}")
    print(f"  Size: {wheel_size / (1024**3):.2f} GB ({wheel_size / (1024**2):.0f} MB)")
    print(f"  Files: {len(package_files)} package + {len(dist_info_files) + 1} dist-info")
    return wheel_path


def main():
    parser = argparse.ArgumentParser(
        description="Package compiled vLLM into a distributable wheel"
    )
    parser.add_argument(
        "--source-dir",
        type=Path,
        default=Path(r"E:\AgentNate\vllm-source"),
        help="Root directory containing the vllm/ package (default: E:\\AgentNate\\vllm-source)",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("dist"),
        help="Output directory for the wheel (default: dist)",
    )
    args = parser.parse_args()

    build_wheel(args.source_dir, args.output_dir)


if __name__ == "__main__":
    main()
